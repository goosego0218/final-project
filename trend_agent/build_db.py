import os
import re
import json
from typing import List, Tuple

from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from dotenv import load_dotenv


load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MARKDOWN_DIR = r"C:\Users\user\Desktop\trend\markdown_files"
CHROMA_PERSIST_DIR = "./chroma_db_test"

def extract_urls_from_markdown(markdown_text: str) -> Tuple[str, List[dict], List[dict]]:
    """
    ë§ˆí¬ë‹¤ìš´ì—ì„œ ì´ë¯¸ì§€ URLê³¼ ë§í¬ URL ì¶”ì¶œ
    
    Returns:
        (ì •ë¦¬ëœ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸, ë§í¬ ë¦¬ìŠ¤íŠ¸)
    """
    
    images = []
    links = []
    
    # ì´ë¯¸ì§€: ![ì„¤ëª…](URL)
    image_pattern = r'!\[(.*?)\]\((.*?)\)'
    for match in re.finditer(image_pattern, markdown_text):
        images.append({
            "description": match.group(1),
            "url": match.group(2)
        })
    
    # ë§í¬: [í…ìŠ¤íŠ¸](URL)
    link_pattern = r'(?<!!)\[([^\]]+)\]\(([^\)]+)\)'
    for match in re.finditer(link_pattern, markdown_text):
        links.append({
            "text": match.group(1),
            "url": match.group(2)
        })
    
    # í…ìŠ¤íŠ¸ ì •ë¦¬
    cleaned = markdown_text
    cleaned = re.sub(r'!\[(.*?)\]\((.*?)\)', r'[ì´ë¯¸ì§€: \1]', cleaned)
    cleaned = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'[ì°¸ê³ : \1]', cleaned)
    
    return cleaned, images, links


from langchain_community.vectorstores import Chroma

def create_vectorstore(markdown_dir: str, save_path: str = "./chroma_db_test"):
    """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë“¤ì„ Chroma ë²¡í„° DBë¡œ ë³€í™˜"""
    
    print("\n" + "="*60)
    print("Chroma ë²¡í„° DB ìƒì„±")
    print("="*60)
    
    # URL ë§¤í•‘ ë¡œë“œ
    mapping_file = os.path.join(markdown_dir, "url_mapping.json")
    with open(mapping_file, "r", encoding="utf-8") as f:
        url_mapping = json.load(f)
    
    # íŒŒì¼ ë¡œë“œ
    print("\nğŸ“‚ STEP 1: íŒŒì¼ ë¡œë”©...")
    loader = DirectoryLoader(
        markdown_dir,
        glob="*.md",
        loader_cls=TextLoader,
        loader_kwargs={"encoding": "utf-8"}
    )
    documents = loader.load()
    print(f"   âœ“ {len(documents)}ê°œ íŒŒì¼")
    
    # ì´ë¯¸ì§€/ë§í¬ ì¶”ì¶œ
    print("\nğŸ”— STEP 2: URL ì¶”ì¶œ...")
    for doc in documents:
        filename = os.path.basename(doc.metadata.get("source", ""))
        
        # URL ì¶”ì¶œ ë° í…ìŠ¤íŠ¸ ì •ë¦¬
        cleaned, images, links = extract_urls_from_markdown(doc.page_content)
        doc.page_content = cleaned
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        doc.metadata.update({
            "filename": filename,
            "source_url": url_mapping.get(filename, ""),
            "images": json.dumps(images, ensure_ascii=False),
            "links": json.dumps(links, ensure_ascii=False),
            "image_count": len(images),
            "link_count": len(links),
        })
        
        print(f"   - {filename}: ì´ë¯¸ì§€ {len(images)}ê°œ, ë§í¬ {len(links)}ê°œ")
    
    # í—¤ë” ë¶„í• 
    print("\nâœ‚ï¸  STEP 3: í—¤ë” ë¶„í• ...")
    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
    ]
    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)
    
    md_splits = []
    for doc in documents:
        splits = markdown_splitter.split_text(doc.page_content)
        for split in splits:
            split.metadata.update(doc.metadata)
        md_splits.extend(splits)
    
    print(f"   âœ“ {len(md_splits)}ê°œ ì„¹ì…˜")
    
    # í¬ê¸° ì¡°ì • + Overlap
    print("\nâœ‚ï¸  STEP 4: í¬ê¸° ì¡°ì • + Overlap...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
    )
    all_splits = text_splitter.split_documents(md_splits) 
    # chunk_id ì¶”ê°€
    for idx, split in enumerate(all_splits):
        split.metadata["chunk_id"] = idx
    
    print(f"   âœ“ {len(all_splits)}ê°œ ì²­í¬")
    
    # Chroma ë²¡í„° DB ìƒì„±
    print("\nğŸ”„ STEP 5: Chroma DB ìƒì„±...")
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma.from_documents(
        documents=all_splits,
        embedding=embeddings,
        persist_directory=save_path  # ì €ì¥ ê²½ë¡œ
    )
    
    print(f"   âœ“ ì €ì¥: {save_path}/")
    print("\nâœ… ì™„ë£Œ!")
    
    return vectorstore

print("âœ… ë²¡í„° DB ìƒì„± í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ (Chroma)")

if __name__ == "__main__":
    vectorstore = create_vectorstore(MARKDOWN_DIR, save_path=CHROMA_PERSIST_DIR)